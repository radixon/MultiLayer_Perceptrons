# XOR
The mean squared error loss function is chosen to simplify this example; however, MSE usually is not an appropriate cost function for modeling binary data. The MSE loss function is <br />
                              &emsp;   ğ½(ğœ½)= 14Î£(ğ‘“âˆ—(ğ’™)âˆ’ğ‘“(ğ’™;ğœ½))2ğ’™âˆˆğ• <br /> <br />
                                 
The form of the model, ğ‘“(ğ’™;ğœ½), is chosen to be linear, with ğœ½ consisting of ğ and ğ‘. Thus the model is defined to be <br />
                                 ğ‘“(ğ’™;ğ’˜,ğ‘)=ğ’™ğ‘‡ğ +ğ‘ <br /> <br />
                                 
Minimize ğ½(ğœƒ) in closed form with respect to ğ and ğ‘ using the normal equations. Solving the normal equations obtains ğ =0 and ğ‘ = 12â„ as shown here:<br />
![MLP](https://user-images.githubusercontent.com/59415488/176250162-d17a2d7a-a147-4a0d-ab5f-18a769bffd52.jpg)

The XOR feedforward network has one hidden layer containing two hidden units. The feedforward network has a vector of hidden units ğ’‰ that are computed by a function ğ‘“(1)(ğ’™;ğ‘¾,ğ’„). The values of these hidden units are then used as the input for a second layer, which is the output layer of this network. The output layer is applied to ğ’‰ rather than to ğ’™. The network now contains two functions chained together:<br /> 
                                 ğ’‰ = ğ‘“(1)(ğ’™;ğ‘¾,ğ’„) and ğ‘¦ = ğ‘“(2)(ğ’‰;ğ,ğ‘)<br /> <br />

with the complete model being<br /> 
                                 ğ‘“(ğ’™;ğ‘¾,ğ’„,ğ,ğ‘) = ğ‘“(2)(ğ‘“(1)(ğ’™)) <br /> <br />
                                
A nonlinear function must be used to describe the features. The majority of neural networks do so using an affine transformation controlled by learned parameters, followed by a fixed, nonlinear function called an activation function. In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU defined by the activation function ğ‘”(ğ‘§)=max {0,ğ‘§}. Now the complete network is specified as <br /> 
                                ğ‘“(ğ’™;ğ‘¾,ğ’„,ğ,ğ‘) = ğğ‘‡max{0,ğ‘¾ğ‘‡ğ’™ +ğ‘}+ğ‘ <br /> <br />
                                
![MLP 02](https://user-images.githubusercontent.com/59415488/176252289-a58aeac7-0463-460d-b12d-0f8387f0079e.jpg)
